{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyragosa/Deep-Learning-with-Pytorch/blob/main/Lesson4/Homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vit Transformer"
      ],
      "metadata": {
        "id": "mfCxjGV5oTiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Модель"
      ],
      "metadata": {
        "collapsed": false,
        "id": "D-FMYv-jwLiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "3hL_6WoCwLiX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 10])\n",
            "tensor([[-2.5334, -0.9565, -0.6687, -0.2604, -0.7144,  0.8632, -0.8125, -0.4398,\n",
            "          0.7745, -0.1242],\n",
            "        [-0.7363,  0.6791, -0.3438,  0.5119,  0.5442,  0.0076,  0.4123, -0.4552,\n",
            "         -0.0984, -0.9653],\n",
            "        [ 0.6041, -0.8996, -0.1496,  0.7702, -0.3284,  3.1291, -0.8884, -0.5011,\n",
            "         -0.0531,  0.4702],\n",
            "        [ 0.9984, -1.4029,  0.2687, -0.2648, -0.4529, -0.4318, -0.6493, -0.2855,\n",
            "          1.6445,  2.0167],\n",
            "        [-0.1352,  0.2940,  0.1175,  0.2456, -0.8673, -0.4940,  0.8356,  2.2461,\n",
            "         -1.4129,  1.5931]])\n"
          ]
        }
      ],
      "source": [
        "# Смоделируем данные\n",
        "\n",
        "n_features = 10  # Количество признаков\n",
        "n_classes = 3  # Количество классов\n",
        "batch_size = 5 \n",
        "\n",
        "data = torch.randn((batch_size, n_features))\n",
        "print(data.shape)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "GVgNwDyzwLiY",
        "outputId": "a327662e-f7cb-4637-c69c-11966cadbd5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "# Зададим простую модель\n",
        "model = nn.Linear(n_features, n_classes)"
      ],
      "metadata": {
        "id": "GKVgvWaawLiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[ 0.9367, -0.2060, -0.0400],\n",
            "        [ 0.9351,  0.1815,  0.2405],\n",
            "        [ 0.4786,  0.2207, -1.3682],\n",
            "        [-1.0627, -0.2106, -0.6474],\n",
            "        [ 0.0065, -0.5155, -0.4240]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Применим модель к вектору\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "PyfujdNcwLia",
        "outputId": "7e6abb3e-2139-4c45-969e-ef63dba4a17f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "# Модель как наследник nn.Module\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin = nn.Linear(n_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)"
      ],
      "metadata": {
        "id": "oCsGwzuRwLib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[-0.9749,  0.2739, -0.2506],\n",
            "        [ 0.1815, -0.0888, -0.0780],\n",
            "        [-0.7308,  1.3065, -0.7929],\n",
            "        [ 0.1040,  0.9197, -0.4299],\n",
            "        [ 0.0714, -1.0181,  1.3724]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Попробуем применить модель в виде класса к данным\n",
        "model = SimpleNN(n_features, n_classes)\n",
        "\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "AJvKzV5ewLic",
        "outputId": "d97c3064-1969-414f-8293-0cde173758ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 5, 3]              33\n",
            "================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "model = SimpleNN(n_features, n_classes).cuda()\n",
        "\n",
        "# 5, 10\n",
        "input_size = (batch_size, n_features)\n",
        "print(summary(model, input_size))"
      ],
      "metadata": {
        "id": "3I-SqCkrwLid",
        "outputId": "d40ca226-fb56-4135-f4a0-97c471e423e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[ 0.2390, -0.0312,  0.2191],\n",
            "        [ 0.1500, -0.0242, -0.4466],\n",
            "        [ 0.9750,  0.0896,  0.6038],\n",
            "        [-0.1699, -0.6341,  1.0429],\n",
            "        [-0.2059, -0.2868, -0.4787]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Модель как sequential\n",
        "model = nn.Sequential(nn.Linear(n_features, n_classes))\n",
        "\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "YIBvYYBewLie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00071259-3fbc-4ba9-d860-609e35b0ad17"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[ 1.0925, -0.3466,  0.8294],\n",
            "        [ 0.2715, -0.3599,  0.6825],\n",
            "        [-0.2782, -0.0437, -0.1845],\n",
            "        [ 0.1277,  0.7062, -0.8107],\n",
            "        [-0.5537,  0.2513,  0.1564]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Модель как nn.ModuleList\n",
        "\n",
        "model = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
        "\n",
        "# answer = model(data)\n",
        "# print(answer.shape)\n",
        "# print(answer)\n",
        "\n",
        "answer = model[0](data)\n",
        "print(answer.shape)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "uBwzYlptwLif",
        "outputId": "33fb6e74-8412-43eb-f1b3-63224df9f86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "# Проверим параметры модели\n",
        "class ParametersCheck(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin = nn.Linear(n_features, n_classes)\n",
        "        self.seq = nn.Sequential(nn.Linear(n_features, n_classes))\n",
        "        self.module_list = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
        "        self.list_of_layers = [nn.Linear(n_features, n_classes)]\n"
      ],
      "metadata": {
        "id": "m5fE17SRwLig"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Параметр #1.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #2.\n",
            "\ttorch.Size([3])\n",
            "Параметр #3.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #4.\n",
            "\ttorch.Size([3])\n",
            "Параметр #5.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #6.\n",
            "\ttorch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "model = ParametersCheck(n_features, n_classes)\n",
        "\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    print(f'Параметр #{i + 1}.')\n",
        "    print(f'\\t{param.shape}')"
      ],
      "metadata": {
        "id": "pzvFgyhHwLih",
        "outputId": "67286616-89d3-4b85-9820-30274f4e0d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9_ccpqgpwLih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1J5TvycDPs8pzfvlXvtO5MCFBy64yp9Fa)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "O9Ck2xnvwLii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFzQd5YDEbas",
        "outputId": "5055850f-5b87-4b57-963b-e6e2a203efc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "khe7vy_ZwLii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-01.png)"
      ],
      "metadata": {
        "id": "cbPI9vsXDZH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1. Patch Embedding, CLS Token, Position Encoding"
      ],
      "metadata": {
        "id": "I7Au2Fd1FZbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-02.png)"
      ],
      "metadata": {
        "id": "YjbKwA7lGY3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input image `B, C, H, W`\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "# 2D conv\n",
        "conv = nn.Conv2d(3, 768, 16, 16)\n",
        "conv(x).reshape(-1, 196).transpose(0,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tH4Nb22GeuS",
        "outputId": "7cc272fb-1adb-4239-dffd-80f69453da24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embeddings = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        patches = self.patch_embeddings(image).flatten(2).transpose(1, 2)\n",
        "        \n",
        "        return patches"
      ],
      "metadata": {
        "id": "WVwf4n1bwLik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embed = PatchEmbedding()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "patch_embed(x).shape "
      ],
      "metadata": {
        "id": "E57UzPBuE4qi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c087da-ea49-4f7e-a2d0-a2ec69e5439d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-03.png)"
      ],
      "metadata": {
        "id": "JVUm-TJFGm6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 2. Transformer Encoder"
      ],
      "metadata": {
        "id": "rUxuB53PFv1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/ViT.png)"
      ],
      "metadata": {
        "id": "vkklM-fqFpa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-07.png)"
      ],
      "metadata": {
        "id": "G34WzminccX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ACAqbCivDGsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        # Linear Layers\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        \n",
        "        # Activation(s)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VPQts2WWdeYQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 197,768)\n",
        "mlp = MLP(768, 3072, 768)\n",
        "out = mlp(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxxcPoMf7IW",
        "outputId": "cd176f06-f58d-419e-d99b-241309028c49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., out_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "        self.out_drop = nn.Dropout(out_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Out projection\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.out(x)\n",
        "        x = self.out_drop(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4QnAW3rSc2OZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-08.png)"
      ],
      "metadata": {
        "id": "S_vgvLDbcjvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "# attn = attn.softmax(dim=-1)"
      ],
      "metadata": {
        "id": "OukFkeXzdFpB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "attention = Attention(768, 8)\n",
        "out = attention(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NeRHHJAgg5R",
        "outputId": "a736e137-7636-4140-d75b-a5976f31b0da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalization\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Attention\n",
        "        self.attn = Attention(dim, \n",
        "                              num_heads=num_heads, \n",
        "                              qkv_bias=False,\n",
        "                              attn_drop=0., \n",
        "                              out_drop=0.)\n",
        "\n",
        "        # Dropout\n",
        "        self.drop_path = Transformer(drop_rate) if drop_rate > 0. else nn.Identity()\n",
        "\n",
        "        # Normalization\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # MLP\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attetnion\n",
        "        x += self.drop_path(self.attn(self.norm1(x)))\n",
        "\n",
        "        # MLP\n",
        "        x += self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "K6e8y_YvwLik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = Block(768, 8)\n",
        "out = attention(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aMihgfEhyql",
        "outputId": "70de3449-aa63-4dd6-9943-2beac17031fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В оригинальной реализации теперь используется [DropPath](https://github.com/rwightman/pytorch-image-models/blob/e98c93264cde1657b188f974dc928b9d73303b18/timm/layers/drop.py)"
      ],
      "metadata": {
        "id": "BPBmiO5FhoN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b1uO18VTwLil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = Transformer(12, 768)\n",
        "out = attention(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIfp984oiBqc",
        "outputId": "0f07be2f-37d5-4088-ea4a-26b983cc83bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://amaarora.github.io/images/vit-06.png)"
      ],
      "metadata": {
        "id": "GqUxpyv3cwNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., \n",
        "                 qkv_bias=False, drop_rate=0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        # Присвоение переменных\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        \n",
        "        \n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        self.patch_embed = PatchEmbedding(img_size=img_size, \n",
        "                                          patch_size=patch_size, \n",
        "                                          in_chans=in_chans, \n",
        "                                          embed_dim=embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = Transformer(\n",
        "            depth=depth,\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            drop_rate=drop_rate,\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  \n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        x = self.transformer.forward(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Y9gyxdqQeFs6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 224, 224)\n",
        "vit = ViT()\n",
        "out = vit(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lGhne8kjeYs",
        "outputId": "f088ecb3-25da-4dab-b968-069cc2253024"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание"
      ],
      "metadata": {
        "id": "4QbFtayBkp-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Выбрать датасет для классификации изображений с размерностью 64x64+ \n",
        "2. Обучить ViT на таком датасете.\n",
        "3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n",
        "\n",
        "\n",
        "Примечание:\n",
        "- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n",
        "- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."
      ],
      "metadata": {
        "id": "6nZbwbK9kskc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, loss, optimizer, loader, device='cpu', num_epochs=1):\n",
        "    model.to(device)\n",
        "    loss.to(device)\n",
        "    \n",
        "    loss_history = []\n",
        "\n",
        "    with torch.cuda.device(device):\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for x, y in tqdm(loader, desc=f'Epoch {epoch+1}'):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                output = model(x)\n",
        "                batch_loss = loss(output, y)\n",
        "                batch_loss = batch_loss.detach()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += batch_loss.item()\n",
        "            \n",
        "            epoch_loss /= len(loader)\n",
        "            loss_history.append(epoch_loss)\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ViT()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "transform = ToTensor()\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "loss_history = train(model, loss, optimizer, train_loader, device=device, num_epochs=10)\n",
        "\n",
        "print(f'Final Loss: {loss_history[-1]}')"
      ],
      "metadata": {
        "id": "MLgPAPrvyf2l"
      },
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}