{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI77oWjeb2anui8wLdqh/8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef1a3b1312c7483693f6105a5f2368a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c77dc6a013d4d21816ba39cd915f1d5",
              "IPY_MODEL_a40a5067de3d4e9ab15889aa1685efe1",
              "IPY_MODEL_5e658135b71141e1872e9ecaa9abfd63"
            ],
            "layout": "IPY_MODEL_a8352880c4c9409b96665ddc3fa9091f"
          }
        },
        "6c77dc6a013d4d21816ba39cd915f1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88039ba658b4bba8170242f2f19b688",
            "placeholder": "​",
            "style": "IPY_MODEL_474536f67b664669a1b8d194cf9415ce",
            "value": ""
          }
        },
        "a40a5067de3d4e9ab15889aa1685efe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e52b4f190a7e4aaf8653f720279253b2",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74e813501ec7482daf4d1ae81c5c9780",
            "value": 300
          }
        },
        "5e658135b71141e1872e9ecaa9abfd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7993fddd9ad348e49a7954c2b95bc808",
            "placeholder": "​",
            "style": "IPY_MODEL_e114821d8edb4598bc1198e9201fb2bc",
            "value": " 320/? [00:30&lt;00:00, 16.87it/s]"
          }
        },
        "a8352880c4c9409b96665ddc3fa9091f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88039ba658b4bba8170242f2f19b688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "474536f67b664669a1b8d194cf9415ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e52b4f190a7e4aaf8653f720279253b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e813501ec7482daf4d1ae81c5c9780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7993fddd9ad348e49a7954c2b95bc808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e114821d8edb4598bc1198e9201fb2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyragosa/Deep-Learning-with-Pytorch/blob/main/Lesson3/VanGoghStyleTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель"
      ],
      "metadata": {
        "id": "4kPY1T1kMPbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import copy\n",
        "\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "fBtRsG3XMQuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3077d997-87b8-45b8-dd03-8ece5049c872"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.8/dist-packages (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.25.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.8/dist-packages (from gradio) (22.1.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from gradio) (3.17)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (from gradio) (0.89.1)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.8/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from gradio) (4.4.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.8/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: starlette==0.22.0 in /usr/local/lib/python3.8/dist-packages (from fastapi->gradio) (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (0.16.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.3)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart->gradio) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (4.0.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (0.14.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.8/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dptpq7qvMOrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453558a5-c23f-4121-f4c4-48037f64f48b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем использовать алгоритм, предложенный Леоном Гатисом.\n",
        "\n",
        "Основаная идея:\n",
        "\n",
        "Взять исходное изображение и рассматривать его пиксели, как настраиваемые параметры в алгоритме градиентного спуска. Критерий качества должен быть выбран так, чтобы он уменьшался по мере приближения исходного изображения к желаемому стилизованному.\n",
        "\n",
        "В качестве оценки качеаства используются 2 критерия:\n",
        "- степень соответствия преобразованного изображения исходному;\n",
        "- степень стилизации преобразованного изображения. \n",
        "\n",
        "В решении используется сверточная нейросеть VGG19, обученая на базе изображений ImageNet.\n",
        "\n",
        "Вот как выглядит VGG19 внутри\n",
        "\n",
        "![VGG Layers](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)"
      ],
      "metadata": {
        "id": "1szp8yALN7x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target, ):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = StyleLoss.gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(StyleLoss.gram_matrix(input), self.target)\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def gram_matrix(input):\n",
        "        a, b, c, d = input.size()\n",
        "        features = input.view(a * b, c * d)\n",
        "        return torch.mm(features, features.t()).div(a * b * c * d)\n",
        "\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n"
      ],
      "metadata": {
        "id": "oizb8KQeXIXb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = transforms.Compose([transforms.Resize(512), transforms.ToTensor()])\n",
        "unloader = transforms.ToPILImage()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def open_image(url):\n",
        "    \"\"\"\n",
        "    With requests, receives and returns an image\n",
        "    :param url: the address of the image to upload should, if possible, start with http\n",
        "    :return: PIL type image\n",
        "    \"\"\"\n",
        "    print(type(url))\n",
        "    if isinstance(url, str) and url.startswith(\"http\"):\n",
        "        response = requests.get(url)\n",
        "        return Image.open(BytesIO(response.content))\n",
        "    return Image.open(url)\n",
        "\n",
        "\n",
        "def save_image(tensor, filename):\n",
        "    \"\"\"\n",
        "    Saves the image to the file specified in the parameter\n",
        "    :param tensor: Image in the form of a tensor\n",
        "    :param filename: The name of the file to save the image\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    image.save(filename)\n",
        "\n",
        "\n",
        "def image_loader(image_name, im_size):\n",
        "    \"\"\"\n",
        "    Converts the image to RGB format and leads to the size specified in the parameters\n",
        "    :param im_size: Image size tuple (width, height)\n",
        "    :param image_name: Name of the image to convert\n",
        "    :return: A picture in the form of a tensor with Float data\n",
        "    \"\"\"\n",
        "    image = loader(image_name.resize(im_size).convert('RGB')).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "\n",
        "def image_unloader(image_name):\n",
        "    \"\"\"\n",
        "    Converts the image to RGB format and leads to the size specified in the parameters\n",
        "    :param image_name: Name of the image to convert\n",
        "    :return: A picture in the form of a tensor with Float data\n",
        "    \"\"\"\n",
        "    image_name = image_name.squeeze(0)\n",
        "    image_name = unloader(image_name)\n",
        "    return image_name\n",
        "\n",
        "\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std, style_img, content_img,\n",
        "                               content_layers='conv_5',\n",
        "                               style_layers=('conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5')):\n",
        "    \"\"\"\n",
        "\n",
        "    :param cnn:\n",
        "    :param normalization_mean:\n",
        "    :param normalization_std:\n",
        "    :param style_img:\n",
        "    :param content_img:\n",
        "    :param content_layers:\n",
        "    :param style_layers:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = 'conv_{}'.format(i)\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = 'relu_{}'.format(i)\n",
        "\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = 'pool_{}'.format(i)\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = 'bn_{}'.format(i)\n",
        "        else:\n",
        "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "\n",
        "    model = model[:(i + 1)]\n",
        "\n",
        "    return model, style_losses, content_losses\n",
        "\n",
        "\n",
        "def get_input_optimizer(input_img):\n",
        "    optimizer = optim.LBFGS([input_img])\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def run_style_transfer(cnn, content_img, style_img, input_img,\n",
        "                       normalization_mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "                       normalization_std=torch.tensor([0.229, 0.224, 0.225]), num_steps=300,\n",
        "                       style_weight=1000000, content_weight=1):\n",
        "    \"\"\"\n",
        "\n",
        "    :param cnn:\n",
        "    :param content_img:\n",
        "    :param style_img:\n",
        "    :param input_img:\n",
        "    :param size:\n",
        "    :param normalization_mean:\n",
        "    :param normalization_std:\n",
        "    :param num_steps:\n",
        "    :param style_weight:\n",
        "    :param content_weight:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('Building the style transfer model..')\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(cnn, normalization_mean.to(device),\n",
        "                                                                     normalization_std.to(device), style_img,\n",
        "                                                                     content_img)\n",
        "    input_img.requires_grad_(True)\n",
        "    model.requires_grad_(False)\n",
        "\n",
        "    optimizer = get_input_optimizer(input_img)\n",
        "\n",
        "    i = [0]\n",
        "    progress_bar = tqdm(total=num_steps)\n",
        "    while i[0] <= num_steps:\n",
        "        def closure():\n",
        "            with torch.no_grad():\n",
        "                input_img.clamp_(0, 1)\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = sum(sl.loss for sl in style_losses) * style_weight\n",
        "            content_score = sum(cl.loss for cl in content_losses) * content_weight\n",
        "            loss = style_score + content_score\n",
        "            loss.backward()\n",
        "            progress_bar.update(1)\n",
        "            i[0] += 1\n",
        "            return style_score + content_score\n",
        "        optimizer.step(closure)\n",
        "    with torch.no_grad():\n",
        "        input_img.clamp_(0, 1)\n",
        "    return input_img\n",
        "\n",
        "\n",
        "def interfence(style_img, content_img, size, numb_step):\n",
        "    \"\"\"\n",
        "\n",
        "    :param numb_step:number of steps in training\n",
        "    :param size: size of image\n",
        "    :param style_img:\n",
        "    :param content_img:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    size = int(size)\n",
        "    style = image_loader(style_img, (size, size))\n",
        "    content = image_loader(content_img, (size, size))\n",
        "    image = content.clone()\n",
        "    output = run_style_transfer(models.vgg19(pretrained=True).features.to(device).eval(), content, style, image,\n",
        "                                num_steps=numb_step)\n",
        "\n",
        "    output = image_unloader(output)\n",
        "    return output \n"
      ],
      "metadata": {
        "id": "MwR77EyLXNwN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "castle = \"https://raw.githubusercontent.com/vyragosa/Deep-Learning-with-Pytorch/main/Lesson3/content.jpg\"\n",
        "\n",
        "\n",
        "night = \"https://raw.githubusercontent.com/vyragosa/Deep-Learning-with-Pytorch/main/Lesson3/style.jpg\"\n",
        "\n",
        "\n",
        "#open image from url and save it. Get string of image path\n",
        "content = Image.open(requests.get(castle, stream=True).raw).convert('RGB').save('content.jpg')\n",
        "style= Image.open(requests.get(night, stream=True).raw).convert('RGB').save('style.jpg')\n",
        "\n",
        "ex = [\"style.jpg\", \"content.jpg\"]\n",
        "\n",
        "app = gr.Interface(interfence, inputs=[gr.inputs.Image(type=\"pil\", label=\"Style Image\"),\n",
        "                                       gr.inputs.Image(type=\"pil\", label=\"Content Image\"),\n",
        "                                       gr.Radio([\"128\", \"256\", \"512\", \"1024\"], label=\"Size\"),\n",
        "                                       gr.Slider(50, 500, step=50, label=\"Steps\")],\n",
        "                   outputs=gr.outputs.Image(type=\"pil\"), title=\"Style Transfer\",\n",
        "                   description=\"Upload your style and content image and get the output image\",\n",
        "                   examples=[ex])\n",
        "app.launch(share=True, debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "97WN8hKtXTK1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903,
          "referenced_widgets": [
            "ef1a3b1312c7483693f6105a5f2368a7",
            "6c77dc6a013d4d21816ba39cd915f1d5",
            "a40a5067de3d4e9ab15889aa1685efe1",
            "5e658135b71141e1872e9ecaa9abfd63",
            "a8352880c4c9409b96665ddc3fa9091f",
            "a88039ba658b4bba8170242f2f19b688",
            "474536f67b664669a1b8d194cf9415ce",
            "e52b4f190a7e4aaf8653f720279253b2",
            "74e813501ec7482daf4d1ae81c5c9780",
            "7993fddd9ad348e49a7954c2b95bc808",
            "e114821d8edb4598bc1198e9201fb2bc"
          ]
        },
        "outputId": "7927832a-ab22-4b46-ac44-fc56993c299d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gradio/inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.8/dist-packages/gradio/outputs.py:43: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://58e91fb5-b3e9-406f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://58e91fb5-b3e9-406f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the style transfer model..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-41e1c39d9151>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
            "<ipython-input-3-41e1c39d9151>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef1a3b1312c7483693f6105a5f2368a7"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}